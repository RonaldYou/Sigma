{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonaldYou/Sigma/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFbfD3qhb-mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra9SrumScbtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import io\n",
        "import datetime, os\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.lib.io.tf_record import TFRecordWriter\n",
        "from tensorflow.python.keras.callbacks import TensorBoard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaJPca-hvU0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrf7E0-TUacK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd gdrive/My\\ Drive/IgnitionHacks/Sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSeA0Vy3dZwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------------\n",
        "# NLP Model that trains for sentiment analysis\n",
        "#\n",
        "# 08/22/2020\n",
        "# Kenneth Ruan\n",
        "# Ignition Hacks 2020\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Link to the cleaned data in github\n",
        "# data_url = 'https://raw.githubusercontent.com/RonaldYou/Sigma/master/clean_data.csv'\n",
        "\n",
        "# Loading the data from the url\n",
        "# dataframe = pd.read_csv(data_url)\n",
        "\n",
        "# Loading from local disk\n",
        "dataframe = pd.read_csv('clean_data.csv')\n",
        "\n",
        "# Storing the data from the dataframe\n",
        "ids = dataframe['ID']\n",
        "users = dataframe['User']\n",
        "tweets = dataframe['text']\n",
        "sentiment = dataframe['Sentiment']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKCdMzMiJSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine the amount of data used for training/validation\n",
        "DATA_LENGTH = len(tweets)\n",
        "TRAINING_RATIO = 0.8\n",
        "TRAINING_INDEX = int(DATA_LENGTH*TRAINING_RATIO)\n",
        "\n",
        "# Split data\n",
        "training_tweets = tweets[:TRAINING_INDEX]\n",
        "testing_tweets = tweets[TRAINING_INDEX:]\n",
        "\n",
        "training_labels = sentiment[:TRAINING_INDEX]\n",
        "testing_labels = sentiment[TRAINING_INDEX:]\n",
        "\n",
        "# Stores all unique words to determine vocabulary size\n",
        "vocab = set()\n",
        "\n",
        "for entry in training_tweets[:TRAINING_INDEX]:\n",
        "  for word in entry.split(\" \"):\n",
        "    vocab.add(word)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al-MPe3qg4ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters for the Embedding Layer in the model\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)\n",
        "embedding_dim = 32\n",
        "MAX_TWEET_LENGTH = 140  # Maximum of 140 words if a tweet is 280 characters\n",
        "\n",
        "# Constants for Tokenizer\n",
        "OOV_TOKEN = '<OOV>' # Generic character used as a placeholder for words 'Outside Of Vocabulary\"\n",
        "PAD_TYPE = 'pre'\n",
        "TRUNC_TYPE = 'pre'\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Tokenizer\n",
        "#\n",
        "# The neural network takes input in the form of numbers representing features\n",
        "# of a body of text. Tokenizer is utilized here to map all of the words to a \n",
        "# number. Each tweet is then changed into a sequence of numbers and padded\n",
        "# for a consistent shape.\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=OOV_TOKEN)\n",
        "tokenizer.fit_on_texts(training_tweets)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_tweets)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=MAX_TWEET_LENGTH,\n",
        "                                padding=PAD_TYPE, truncating=TRUNC_TYPE)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_tweets)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=MAX_TWEET_LENGTH,\n",
        "                                padding=PAD_TYPE, truncating=TRUNC_TYPE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBsbdbXVvUYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert lists to numpy arrays for Tensorflow 2.x compatibility\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZpTmpioCY9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------------\n",
        "# Batches\n",
        "# \n",
        "# When training with a Neural Network, the data is loaded\n",
        "# into RAM for quick access. However, our training dataset\n",
        "# is 800 000 elements so it isn't possible to store all of\n",
        "# those elements inside the RAM simultaneously nor would it\n",
        "# be very efficient.\n",
        "#\n",
        "# Tensorflow offers a solution to this in the form of\n",
        "# a data pipeline. Data pipelines can be thought of as a \n",
        "# physical pipe where elements are passed through at a \n",
        "# controlled rate as to not overwhelm the system.\n",
        "#\n",
        "# In order to create this data pipeline we use TensorFlow's\n",
        "# Dataset module. We batch the data into groups to be fed\n",
        "# into the RAM and randomize the order of elements to prevent\n",
        "# overfitting.\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Conversion to tensors to create dataset\n",
        "training_padded = tf.convert_to_tensor(training_padded)\n",
        "training_labels = tf.convert_to_tensor(training_labels)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((training_padded,training_labels))\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Buffer size to shuffle the dataset\n",
        "#\n",
        "# TF data supports potentially infinite sequences,\n",
        "# so instead of shuffling the whole dataset, a buffer is\n",
        "# maintained to control how much is shuffled\n",
        "# -----------------------------------------------------------\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOtY3PRsuCpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------------\n",
        "# Model Architecture\n",
        "\n",
        "# Embedding - Converts words into vectors in transformed\n",
        "# space to represent the data more meaningfully as opposed\n",
        "# to a 'Bag of Words' where words are mapped to an ID.\n",
        "\n",
        "# Dropout - The dropout layer randomly sets inputs to 0, this\n",
        "# helps introduce some randomness in order to prevent\n",
        "# overfitting\n",
        "\n",
        "# LSTM - Long Short-Term Memory is a layer specialized for \n",
        "# taking into account the context from a sequence. This helps\n",
        "# with tasks such as NLP, as language is very context\n",
        "# dependant.\n",
        "\n",
        "# GlobalAveragePooling1D - Pooling layers are used to reduce\n",
        "# the number of parameters in a model. This helps avoid\n",
        "# overfitting to the training data. We are performing binary\n",
        "# classification so it is only 1-dimensional.\n",
        "\n",
        "# Dense - Dense layers represent a layer of neurons\n",
        "# that is fully connected to the previous layer. The weights\n",
        "# are adjusted during training as with all the other layers.\n",
        "#\n",
        "# Activation Types\n",
        "# \n",
        "# Relu - All negative values become 0, positive values\n",
        "# remain the same.\n",
        "#\n",
        "# Sigmoid - Values are transformed into one that is between\n",
        "# 0.0 to 1.0, used to determine the probability for the final\n",
        "# output\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=MAX_TWEET_LENGTH),\n",
        "  tf.keras.layers.SpatialDropout1D(0.25),\n",
        "  tf.keras.layers.LSTM(50, dropout=0.5, recurrent_dropout=0.3),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3M4l_viIl9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "ea294e03-8e2a-493f-f9e9-0e7bf2be3dc2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 140, 32)           9578368   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 140, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50)                16600     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 9,595,019\n",
            "Trainable params: 9,595,019\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5l0ml1fIiaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------------\n",
        "# Model Callbacks\n",
        "#\n",
        "# The code below is used to setup callbacks in the neural\n",
        "# network. These callbacks provide information and files\n",
        "# that are important for keeping a neural network project\n",
        "# organized and easy-to-use\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Create logs of stats such as loss and accuracy to pass to the TensorBoard\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir=logdir, update_freq = 1250)\n",
        "\n",
        "# Setting up checkpoints so that trained weights can be stored\n",
        "checkpoint_dir = r'training_checkpoints/v3'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIaDBtEruHN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs # Application used to monitor training progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TspWvcuIwyVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 2\n",
        "model.load_weights(tf.train.latest_checkpoint('training_checkpoints/v2'))\n",
        "history = model.fit(data, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1, callbacks=[tensorboard, checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx5-YYl6wMvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the model to load in the trained weights\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=MAX_TWEET_LENGTH),\n",
        "  tf.keras.layers.SpatialDropout1D(0.25),\n",
        "  tf.keras.layers.LSTM(50, dropout=0.5, recurrent_dropout=0.3),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "#Loads the weights in from the most recent save\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# METHOD 1: Loading the data from the url\n",
        "# judge_data_url = 'https://raw.githubusercontent.com/RonaldYou/Sigma/master/clean_jdata.csv'\n",
        "# judge_dataframe = pd.read_csv(data_url)\n",
        "\n",
        "# METHOD 2: Loading data from local disk\n",
        "judge_dataframe = pd.read_csv('clean_jdata.csv')\n",
        "\n",
        "# Storing the data from the dataframe\n",
        "judge_ids = judge_dataframe['ID']\n",
        "judge_users = judge_dataframe['User']\n",
        "judge_tweets = judge_dataframe['text']\n",
        "\n",
        "# Tokenizing the sequences\n",
        "judge_sequences = tokenizer.texts_to_sequences(judge_tweets)\n",
        "judge_padded = pad_sequences(judge_sequences, maxlen=MAX_TWEET_LENGTH,\n",
        "                                padding=PAD_TYPE, truncating=TRUNC_TYPE)\n",
        "\n",
        "# Convert lists into dataset format\n",
        "judge_padded = np.array(np.expand_dims(judge_padded,1))\n",
        "judge_padded = tf.convert_to_tensor(judge_padded)\n",
        "judge_dataset = tf.data.Dataset.from_tensor_slices(judge_padded)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vseKmYsBCy98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data from original csv to format the submission\n",
        "original_data = pd.read_csv('contestant_judgment.csv')\n",
        "og_ids = original_data['ID']\n",
        "og_users = original_data['User']\n",
        "og_tweets = original_data['Text']\n",
        "\n",
        "# Perform predictions on judge dataset\n",
        "p = model.predict(judge_dataset, batch_size=32, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RQ2hwhDGxZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write information to submission.csv\n",
        "import csv\n",
        "\n",
        "with open('submission.csv', 'w', newline='', encoding='utf-8') as newfile:\n",
        "  csvwriter = csv.writer(newfile)\n",
        "  csvwriter.writerow([\"ID\", \"User\", \"Text\", \"Sentiment\"]) #Headers for csv\n",
        " \n",
        "  for idx in range(len(judge_dataset)):\n",
        "    csvwriter.writerow([og_ids[idx], og_users[idx], og_tweets[idx], int(round(p[idx][0]))])\n",
        "    print(idx)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}